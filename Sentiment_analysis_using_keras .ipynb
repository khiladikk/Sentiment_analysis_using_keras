{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lPtzjRBl1mix"
   },
   "outputs": [],
   "source": [
    "#Importing the library (dealing with dataset)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQ_0ydQk1wsc"
   },
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "data= pd.read_csv(\"movie.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ohDTTQwgj745",
    "outputId": "59a2e704-e234-4fc0-dacd-fdeb87d002d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of the dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "8Oc3rgQr15FW",
    "outputId": "adc75a06-5704-4a50-9e45-84908a57be31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pos</td>\n",
       "      <td>films adapted from comic books have had plent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pos</td>\n",
       "      <td>every now and then a movie comes along from a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pos</td>\n",
       "      <td>you ve got mail works alot better than it des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pos</td>\n",
       "      <td>jaws   is a rare film that grabs your atte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pos</td>\n",
       "      <td>moviemaking is a lot like being the general m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   Pos   films adapted from comic books have had plent...\n",
       "1   Pos   every now and then a movie comes along from a...\n",
       "2   Pos   you ve got mail works alot better than it des...\n",
       "3   Pos      jaws   is a rare film that grabs your atte...\n",
       "4   Pos   moviemaking is a lot like being the general m..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 5 rows and columns of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gDc7Qevv3NA0",
    "outputId": "4c4982c9-8c92-4258-caa3-931b91f95b93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for the any missing or null value\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zooTVRRH3RMc"
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset into features and labels\n",
    "features = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWfgGsUP5wYq"
   },
   "outputs": [],
   "source": [
    "label = data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d9TBFOVPkKLU",
    "outputId": "f7aecf60-0dbd-495d-cebe-3985c9ca559b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of the feature\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "zUhU1JTKmQxk",
    "outputId": "929f5daa-6a3c-4140-d27b-1d29ad8a4e6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Processing the dataset before model training\n",
    "\n",
    "# Importing the dependencies\n",
    "import re  #Library to clean the data\n",
    "import nltk  #Natural language tool kit\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords  #for removing stop words\n",
    "from nltk.stem.porter import PorterStemmer  #for stemming purpose\n",
    "\n",
    "\n",
    "#Intialize empty array\n",
    "\n",
    "# To append clean list\n",
    "corpus = []\n",
    "\n",
    "# Processing our dataset\n",
    "for i in range(0, data.shape[0]):\n",
    "    \n",
    "    # Row ith column: 'Text'\n",
    "    text = re.sub('[^a-zA-Z]', ' ', data['text'][i])\n",
    "    \n",
    "    # Converting all the cases to lower\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Splitting to array\n",
    "    text = text.split()\n",
    "    \n",
    "    # Creating porterstemmer object to\n",
    "    # using main stem of each word\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    # Using for-loop to stem each word in the string array to its Ith row\n",
    "    text = [ps.stem(word) for word in text if word in ['not',\"isin't\",\n",
    "              \"ain't\",\"don't\",\"dosen't\"] or not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    # Rejoin all string arrays to create back into a string\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    # Append each string to create array of clean text\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-1CifBepQGC"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# To extract a total number of 500 features\n",
    "# here max_features is an attribute to get better result\n",
    "cv = CountVectorizer(max_features = data.shape[1]+500)\n",
    "\n",
    "# Splitting the dataset into features and label\n",
    "\n",
    "# Features is the dependent varibles\n",
    "features = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "# label has the answer( if the review the positive or negative)\n",
    "labels = data.iloc[:, 0].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split as TTS\n",
    "features_train, features_test, label_train, label_test = TTS(features, label, test_size = 0.20, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6apJ46d8nZRY",
    "outputId": "8cf8b922-4b3e-45b4-947b-9b904d966396"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 502)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of training feature\n",
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UT0KdaoOG2Ou",
    "outputId": "6718e870-4086-485e-dbe8-5a7deedc8f5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing our dependencies for the neural network\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JfKnC0KrG4K1",
    "outputId": "5ad5a440-c826-468d-948b-7518c07e8e03"
   },
   "outputs": [],
   "source": [
    "# Importing the other dependencies\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SJtkKZgroKVA"
   },
   "outputs": [],
   "source": [
    "# We have label data in string format \n",
    "# So, label-Encoding our label dataset for better training\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label_train = le.fit_transform(label_train)\n",
    "label_test = le.transform(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Zh88YGFIorEs",
    "outputId": "af277df2-08a5-4e42-fe7a-feba57e188e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encoded data\n",
    "label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "eWqDZ8evaxhv",
    "outputId": "8d4010dd-581d-417b-df3e-d918e4487c31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\kk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Creating Model\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the Hidden Layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 502))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3434
    },
    "colab_type": "code",
    "id": "rWHPYpdLcEjl",
    "outputId": "85fa3ae7-4d39-4b85-e8ac-188ac802cabe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\kk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 0s 243us/step - loss: 0.6622 - acc: 0.6481\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 0s 100us/step - loss: 0.4776 - acc: 0.8162\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 0s 97us/step - loss: 0.3607 - acc: 0.8550\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 0s 101us/step - loss: 0.3111 - acc: 0.8719\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 0s 103us/step - loss: 0.2757 - acc: 0.8906\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 0s 105us/step - loss: 0.2576 - acc: 0.8994\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 0s 101us/step - loss: 0.2378 - acc: 0.9037\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 0s 109us/step - loss: 0.2280 - acc: 0.9081\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 0s 104us/step - loss: 0.2140 - acc: 0.9169\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 0s 102us/step - loss: 0.2079 - acc: 0.9169\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 0s 98us/step - loss: 0.1951 - acc: 0.9237\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 0s 95us/step - loss: 0.1900 - acc: 0.9244\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 0s 99us/step - loss: 0.1852 - acc: 0.9287\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 0s 108us/step - loss: 0.1772 - acc: 0.9306\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 0s 107us/step - loss: 0.1711 - acc: 0.9375\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 0s 111us/step - loss: 0.1681 - acc: 0.9369\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 0s 101us/step - loss: 0.1635 - acc: 0.9381\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 0s 115us/step - loss: 0.1601 - acc: 0.9369\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 0s 95us/step - loss: 0.1566 - acc: 0.9387\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 0s 106us/step - loss: 0.1478 - acc: 0.9462\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 0s 98us/step - loss: 0.1446 - acc: 0.9431\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 0s 101us/step - loss: 0.1444 - acc: 0.9456\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 0s 101us/step - loss: 0.1423 - acc: 0.9444\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 0s 100us/step - loss: 0.1367 - acc: 0.9537\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 0s 95us/step - loss: 0.1329 - acc: 0.9525\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 0s 104us/step - loss: 0.1330 - acc: 0.9519\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 0s 104us/step - loss: 0.1301 - acc: 0.9481\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 0s 103us/step - loss: 0.1273 - acc: 0.9537\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 0s 99us/step - loss: 0.1219 - acc: 0.9575\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 0s 97us/step - loss: 0.1241 - acc: 0.9550\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 0s 98us/step - loss: 0.1216 - acc: 0.9506\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 0s 103us/step - loss: 0.1131 - acc: 0.9587\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 0s 101us/step - loss: 0.1087 - acc: 0.9625\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 0s 104us/step - loss: 0.1114 - acc: 0.9606\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 0s 103us/step - loss: 0.1055 - acc: 0.9625\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 0s 100us/step - loss: 0.1035 - acc: 0.9631\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.0937 - acc: 0.968 - 0s 111us/step - loss: 0.1046 - acc: 0.9637\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 0s 113us/step - loss: 0.0988 - acc: 0.9644\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 0s 106us/step - loss: 0.0991 - acc: 0.9675\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 0s 119us/step - loss: 0.0958 - acc: 0.9662\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 0s 101us/step - loss: 0.0949 - acc: 0.9656\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 0s 96us/step - loss: 0.0958 - acc: 0.9681\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 0s 100us/step - loss: 0.0916 - acc: 0.9669\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 0s 102us/step - loss: 0.0887 - acc: 0.9706\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 0s 103us/step - loss: 0.0861 - acc: 0.9744\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 0s 100us/step - loss: 0.0822 - acc: 0.9756\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 0s 101us/step - loss: 0.0787 - acc: 0.9800\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 0s 102us/step - loss: 0.0775 - acc: 0.9806\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 0s 97us/step - loss: 0.0771 - acc: 0.9775\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 0s 103us/step - loss: 0.0758 - acc: 0.9806\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 0s 114us/step - loss: 0.0792 - acc: 0.9775\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 0s 101us/step - loss: 0.0706 - acc: 0.9806\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 0s 95us/step - loss: 0.0674 - acc: 0.9825\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 0s 98us/step - loss: 0.0665 - acc: 0.9825\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 0s 97us/step - loss: 0.0646 - acc: 0.9825\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 0s 114us/step - loss: 0.0656 - acc: 0.9831\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 0s 117us/step - loss: 0.0651 - acc: 0.9812\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 0s 111us/step - loss: 0.0573 - acc: 0.9881\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 0s 105us/step - loss: 0.0618 - acc: 0.9806\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 0s 105us/step - loss: 0.0672 - acc: 0.9819\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 0s 113us/step - loss: 0.0551 - acc: 0.9869\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 0s 115us/step - loss: 0.0490 - acc: 0.9919\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 0s 117us/step - loss: 0.0531 - acc: 0.9869\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 0s 116us/step - loss: 0.0501 - acc: 0.9887\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 0s 117us/step - loss: 0.0537 - acc: 0.9850\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 0s 119us/step - loss: 0.0459 - acc: 0.9912\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 0s 111us/step - loss: 0.0439 - acc: 0.9894\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 0s 100us/step - loss: 0.0444 - acc: 0.9925\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 0s 99us/step - loss: 0.0433 - acc: 0.9906\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 0s 98us/step - loss: 0.0406 - acc: 0.9900\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 0s 116us/step - loss: 0.0425 - acc: 0.9925\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 0s 109us/step - loss: 0.0387 - acc: 0.9944\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 0s 115us/step - loss: 0.0352 - acc: 0.9969\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 0s 105us/step - loss: 0.0413 - acc: 0.9875\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 0s 114us/step - loss: 0.0369 - acc: 0.9925\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 0s 112us/step - loss: 0.0461 - acc: 0.9850\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 0s 109us/step - loss: 0.0368 - acc: 0.9912\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 0s 116us/step - loss: 0.0296 - acc: 0.9956\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 0s 116us/step - loss: 0.0345 - acc: 0.9925\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 0s 108us/step - loss: 0.0325 - acc: 0.9956\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 0s 95us/step - loss: 0.0263 - acc: 0.9962\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 0s 113us/step - loss: 0.0236 - acc: 0.9987\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 0s 110us/step - loss: 0.0225 - acc: 0.9987\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 0s 101us/step - loss: 0.0261 - acc: 0.9962\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 0s 103us/step - loss: 0.0239 - acc: 0.9969\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 0s 98us/step - loss: 0.0208 - acc: 0.9975\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 0s 102us/step - loss: 0.0203 - acc: 0.9987\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 0s 117us/step - loss: 0.0198 - acc: 0.9981\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 0s 117us/step - loss: 0.0217 - acc: 0.9975\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 0s 102us/step - loss: 0.0225 - acc: 0.9969\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 0s 119us/step - loss: 0.0270 - acc: 0.9937\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 0s 104us/step - loss: 0.0262 - acc: 0.9937\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 0s 117us/step - loss: 0.0600 - acc: 0.9825\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 0s 100us/step - loss: 0.0257 - acc: 0.9950\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 0s 99us/step - loss: 0.0249 - acc: 0.9937\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 0s 103us/step - loss: 0.0189 - acc: 0.9981\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 0s 119us/step - loss: 0.0131 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 0s 108us/step - loss: 0.0109 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 0s 117us/step - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 0s 119us/step - loss: 0.0105 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21bfcd06438>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training The model\n",
    "classifier.fit(features_train, label_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "dYLyKKodpV5s",
    "outputId": "760375e0-fa2d-4b79-d5d2-05a77744276a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 0s 51us/step\n",
      "['loss', 'acc']\n",
      "[0.007604300444945693, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "# Model accuracy\n",
    "scores = classifier.evaluate(features_train, label_train)\n",
    "print (classifier.metrics_names)\n",
    "print (scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BNUbgTqKpuHQ",
    "outputId": "17e24871-0c48-4184-f4c6-abcab026ac2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy in %\n",
    "print(\"%s: %.2f%%\" % (classifier.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.977751]]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Model\n",
    "\n",
    "new= \"jaws   is a rare film that grabs your attention before it shows you a single image on screen    the movie opens with blackness   and only distant   alien like underwater sounds    then it comes   the first ominous bars of composer john williams  now infamous score    dah dum    from there   director steven spielberg wastes no time   taking us into the water on a midnight swim with a beautiful girl that turns deadly    right away he lets us know how vulnerable we all are floating in the ocean   and once   jaws   has attacked   it never relinquishes its grip    perhaps what is most outstanding about   jaws   is how spielberg builds the movie    he works it like a theatrical production   with a first act and a second act    unlike so many modern filmmakers   he has a great deal of restraint   and refuses to show us the shark until the middle of the second act    until then   he merely suggests its presence with creepy   subjective underwater shots and williams  music    he s building the tension bit by bit   so when it comes time for the climax   the shark s arrival is truly terrifying    he doesn t let us get bored with the imagery    the first act opens with police chief martin brody   roy scheider \"\n",
    "\n",
    "review = re.sub('[^a-zA-Z]', ' ', new)\n",
    "review = review.lower()\n",
    "review = review.split()\n",
    "ps = PorterStemmer()\n",
    "review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "review = ' '.join(review)\n",
    "\n",
    "new = cv.transform([review]).toarray()\n",
    "\n",
    "pred = classifier.predict(new)\n",
    "\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment analysis keras.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
